---
title: "대통령 연설문 예측"
description: |
  TBD
author:
  - name: 박성은 
    url: https://alice3153.github.io/
    affiliation: 명지대학교 기록정보과학전문대학원
    affiliation_url: https://record.mju.ac.kr/
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    toc_depth: 3  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE, 
                      collapse = FALSE,
                      fig.align = "center",
                      tidy.opts = list(width.cutoff = 70), 
                      tidy = TRUE)
knitr::opts_chunk$set(fig.width = 12, fig.height = 9)

library(shiny, warn.conflicts = FALSE)
library(dplyr, warn.conflicts = FALSE)
library(ggplot2, warn.conflicts = FALSE)

xaringanExtra :: use_panelset()
```

# 패키지 로드하기
```{r road, echo=TRUE}
library(tidyverse)
library(tidymodels)
library(text2vec)
library(glmnet)
library(caret)
library(bitTA)
```


# 데이터셋 분리
```{r slice, echo=TRUE}
set.seed(123)
president_split <- rsample::initial_split(president_speech, prop = 7/8, strata = president)

president_smpl <- rsample::testing(president_split)

set.seed(123)
president_split <- initial_split(president_smpl, prop = 0.7, strata = president)

train <- rsample::training(president_split)
test <- rsample::testing(president_split)
```

# Frequency 기반의 DTM 생성
## tokenize 반복기 정의
```{r token, echo=TRUE}
token_fun <- text2vec::word_tokenizer

it_train <- itoken(train$doc, 
                   tokenizer = token_fun, 
                   ids = train$id, 
                   progressbar = FALSE)

it_test <- itoken(test$doc,
                  tokenizer = token_fun, 
                  ids = test$id, 
                  progressbar = FALSE)
```

## Vocabulary 생성
```{r voca, echo=TRUE}
vocab <- create_vocabulary(it_train)
tail(vocab, n = 10)
```

## Document Term Matrix 생성하기
```{r dtm, echo=TRUE}
vectorizer <-  vocab_vectorizer(vocab)

dtm_train_tf <- text2vec::create_dtm(it_train, vectorizer)
dim(dtm_train_tf)

dtm_test_tf <- text2vec::create_dtm(it_test, vectorizer)
dim(dtm_test_tf)
```

# N-Grams 기반의 DTM 생성
## Vocabulary 생성
```{r voca_ngram, echo=TRUE}
vocab_bigram <- create_vocabulary(it_train, ngram = c(1L, 2L))
dim(vocab_bigram)

head(vocab_bigram, n = 10) 
```

## Prune Vocabulary
```{r prune voca, echo=TRUE}
vocab_bigram <- vocab_bigram %>% 
  prune_vocabulary(term_count_min = 10,
                   doc_proportion_max = 0.5)
dim(vocab_bigram)
```

##Documents Term Matrix 생성
```{r create dtm, echo=TRUE}
vectorizer_bigram <- vocab_vectorizer(vocab_bigram)

dtm_train_bigram <- create_dtm(it_train, vectorizer_bigram)
dim(dtm_train_bigram)

dtm_test_bigram  <- create_dtm(it_test, vectorizer_bigram)
dim(dtm_test_bigram)
```

# TF-IDF 기반의 DTM 생성
## DTM의 TF-IDF 변환
```{r tf_dtm, echo=TRUE}
tfidf <- TfIdf$new()

dtm_train_tfidf <- fit_transform(dtm_train_tf, tfidf)
dtm_test_tfidf <- fit_transform(dtm_test_tf, tfidf) 
```

# DTM의 크기 비교
```{r dim, echo=TRUE}
dim(dtm_train_tf)
dim(dtm_train_bigram)
dim(dtm_train_tfidf)
```

# Frequency 기반 모델링
## 모델 생성
```{r model_tf, echo=TRUE}
NFOLDS <- 10

classifier <- cv.glmnet(x = dtm_train_tf, y = train$president, 
                        family = 'multinomial', 
                        alpha = 1,
                        type.measure = "deviance",
                        nfolds = NFOLDS,
                        thresh = 0.001,
                        maxit = 1000)
```

## 모델의 평가
```{r eval_tf, echo=TRUE}
pred_voca <- predict(classifier, dtm_test_tf, type = 'response')[, , 1]
president_voca <- apply(pred_voca, 1, 
                        function(x) colnames(pred_voca)[which(max(x) == x)])

cmat_voca <- confusionMatrix(factor(president_voca), factor(test$president))
cmat_voca
```

#N-Grams 기반 모델링
## 모델 생성
```{r model_ngram, echo=TRUE}
classifier <- cv.glmnet(x = dtm_train_bigram, y = train$president, 
                        family = 'multinomial', 
                        type.measure = "deviance",
                        alpha = 1,                        
                        nfolds = NFOLDS)
```

## 모델의 평가
```{r eval_ngram, echo=TRUE}
pred_bigram <- predict(classifier, dtm_test_bigram, type = 'response')[, , 1]

president_bigram <- apply(pred_bigram, 1, 
                          function(x) colnames(pred_bigram)[which(max(x) == x)])

cmat_bigram <- confusionMatrix(factor(president_bigram), factor(test$president))
cmat_bigram
```

# TF-IDF 기반의 모델
```{r model_tfidf, echo=TRUE}
## 모델 생성
classifier <- cv.glmnet(x = dtm_train_tfidf, y = train$president, 
                        family = 'multinomial', 
                        nfolds = NFOLDS,
                        thresh = 1e-3,
                        maxit = 1e3)

## 모델의 평가
pred_tfidf <- predict(classifier, dtm_test_tfidf, type = 'response')[, , 1]

president_tfidf <- apply(pred_tfidf, 1, 
                         function(x) colnames(pred_tfidf)
                         [which(max(x) == x)])

cmat_tfidf <- confusionMatrix(factor(president_tfidf), factor(test$president))
cmat_tfidf
```

# 모델 성능의 비교
```{r eval_all, echo=TRUE}
accuracy <- rbind(cmat_voca$overall, cmat_bigram$overall, 
                  cmat_tfidf$overall) %>%
  round(3)

data.frame(Method = c("Frequency", "Bigram", "TF-IDF"),
           accuracy) %>%
  arrange(desc(Accuracy)) %>%
  knitr::kable()
```
